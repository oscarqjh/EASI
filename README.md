# EASI

<b>Holistic Evaluation of Multimodal LLMs on Spatial Intelligence</b>

English | [ÁÆÄ‰Ωì‰∏≠Êñá](README_CN.md) 

<p align="center">
    <a href="https://arxiv.org/abs/2508.13142" target="_blank">
        <img alt="arXiv" src="https://img.shields.io/badge/arXiv-EASI-red?logo=arxiv" height="20" />
    </a>
    <a href="https://huggingface.co/spaces/lmms-lab-si/EASI-Leaderboard" target="_blank">
        <img alt="Data" src="https://img.shields.io/badge/%F0%9F%A4%97%20_EASI-Leaderboard-ffc107?color=ffc107&logoColor=white" height="20" />
    </a>
    <a href="https://github.com/EvolvingLMMs-Lab/EASI/blob/main/LICENSE"><img src="https://img.shields.io/github/license/EvolvingLMMs-Lab/EASI?style=flat"></a>
</p>

## TL;DR

- EASI is a unified evaluation suite for Spatial Intelligence in multimodal LLMs.
- EASI supports **two evaluation backends**: [VLMEvalKit](https://github.com/open-compass/VLMEvalKit) and [lmms-eval](https://github.com/EvolvingLMMs-Lab/lmms-eval).
- After installation, you can quickly try a SenseNova-SI model with:

**Using VLMEvalKit backend:**
```bash
python run.py --data MindCubeBench_tiny_raw_qa \
              --model SenseNova-SI-1.3-InternVL3-8B \
              --verbose --reuse --judge extract_matching
```

**Using lmms-eval backend:**
```bash
lmms-eval --model qwen2_5_vl \
          --model_args pretrained=Qwen/Qwen2.5-VL-3B-Instruct \
          --tasks site_bench_image \
          --batch_size 1 \
          --log_samples \
          --output_path ./logs/
```

## Overview

EASI is a unified evaluation suite for Spatial Intelligence. It benchmarks state-of-the-art proprietary and open-source multimodal LLMs across a growing set of spatial benchmarks.

Key features include:

- Supports the evaluation of **state-of-the-art Spatial Intelligence models**.
- Systematically collects and integrates **evolving Spatial Intelligence benchmarks**.
- Provides **two evaluation backends** for flexibility:
  - **VLMEvalKit**: Rich model zoo with built-in judging capabilities.
  - **lmms-eval**: Lightweight, accelerate-based distributed evaluation with extensive task support.

As of [v0.1.5](https://github.com/EvolvingLMMs-Lab/EASI/releases/tag/0.1.5), EASI supports **23 Spatial Intelligence models** and **24 spatial benchmarks**, and the list is continuously expanding. Full details are available at üëâ **[Supported Models & Benchmarks](docs/Support_bench_models.md)**. EASI also provides transparent üëâ **[Benchmark Verification](docs/Benchmark_Verification.md)** against official scores.


## üóìÔ∏è News

üåü **[2026-01-09]** [EASI v0.1.5](https://github.com/EvolvingLMMs-Lab/EASI/releases/tag/0.1.5) is released. Major updates include:
- **Expanded benchmark support**: Added STI-Bench.  
- **Expanded model support**: Added SenseNova-SI-1.1-BAGEL-7B-MoT, SenseNova-SI-1.3-InternVL3-8B.  
- **[Benchmark Verification](docs/Benchmark_Verification.md)** now includes verification details for each newly added benchmark.


For the full release history and detailed changelog, please see üëâ **[Changelog](docs/CHANGELOG.md)**.

## üõ†Ô∏è QuickStart
### Installation

EASI provides two evaluation backends. You can install one or both depending on your needs.

#### Option 1: Local environment (VLMEvalKit backend)
```bash
git clone --recursive https://github.com/EvolvingLMMs-Lab/EASI.git
cd EASI
pip install -e ./VLMEvalKit
```

#### Option 2: Local environment (lmms-eval backend)
```bash
git clone --recursive https://github.com/EvolvingLMMs-Lab/EASI.git
cd EASI
pip install -e ./lmms-eval spacy
# Recommended Dependencies
# Use "torch==2.7.1", "torchvision==0.22.1" in pyproject.toml (this works with most models)
# Install flash-attn for faster inference
pip install flash-attn --no-build-isolation
```

#### Option 3: Docker-based environment
```
bash dockerfiles/EASI/build_runtime_docker.sh

docker run --gpus all -it --rm \
  -v /path/to/your/data:/mnt/data \
  --name easi-runtime \
  vlmevalkit_EASI:latest \
  /bin/bash
```

### Evaluation

EASI supports two evaluation backends. Choose the one that best fits your needs.

---

#### Backend 1: VLMEvalKit

**General command**
```bash
python run.py --data {BENCHMARK_NAME} --model {MODEL_NAME} --judge {JUDGE_MODE} --verbose --reuse 
```
Please refer to the Configuration section below for the full list of available models and benchmarks. See `run.py` for the full list of arguments.

**Example** 

Evaluate `SenseNova-SI-1.3-InternVL3-8B` on `MindCubeBench_tiny_raw_qa`:

```bash
python run.py --data MindCubeBench_tiny_raw_qa \
              --model SenseNova-SI-1.3-InternVL3-8B \
              --verbose --reuse --judge extract_matching
```
This uses regex-based answer extraction. For LLM-based judging (e.g., on SpatialVizBench_CoT), switch to the OpenAI judge:
```bash
export OPENAI_API_KEY=YOUR_KEY
python run.py --data SpatialVizBench_CoT \
              --model {MODEL_NAME} \
              --verbose --reuse --judge gpt-4o-1120
```

---

#### Backend 2: lmms-eval

lmms-eval provides accelerate-based distributed evaluation with support for multi-GPU inference.

**General command**
```bash
lmms-eval --model {MODEL_TYPE} \
          --model_args pretrained={MODEL_PATH} \
          --tasks {TASK_NAME} \
          --batch_size 1 \
          --log_samples \
          --output_path ./logs/
```

**Example: Single GPU**

Evaluate `Qwen2.5-VL-3B-Instruct` on `site_bench_image`:

```bash
lmms-eval --model qwen2_5_vl \
          --model_args pretrained=Qwen/Qwen2.5-VL-3B-Instruct \
          --tasks site_bench_image \
          --batch_size 1 \
          --log_samples \
          --output_path ./logs/
```

**Example: Multi-GPU with accelerate**

```bash
CUDA_VISIBLE_DEVICES=0,1,2,3 accelerate launch \
    --num_processes=4 \
    --num_machines=1 \
    --mixed_precision=no \
    --dynamo_backend=no \
    --main_process_port=12346 \
    -m lmms_eval \
    --model qwen2_5_vl \
    --model_args pretrained=Qwen/Qwen2.5-VL-3B-Instruct,attn_implementation=flash_attention_2 \
    --tasks site_bench_image \
    --batch_size 1 \
    --log_samples \
    --output_path ./logs/
```

**List available tasks**
```bash
lmms-eval --tasks list
```

For more details on lmms-eval usage, refer to the documentation in [lmms-eval/docs/](lmms-eval/docs/), including [model guide](lmms-eval/docs/model_guide.md), [task guide](lmms-eval/docs/task_guide.md), and [run examples](lmms-eval/docs/run_examples.md).

---

### Configuration

#### VLMEvalKit Configuration

**VLM Configuration**: During evaluation, all supported VLMs are configured in `vlmeval/config.py`. Make sure you can successfully infer with the VLM before starting the evaluation with the following command `vlmutil check {MODEL_NAME}`. 

**Benchmark Configuration**: The full list of supported Benchmarks can be found in the official VLMEvalKit documentation [VLMEvalKit Supported Benchmarks](https://aicarrier.feishu.cn/wiki/Qp7wwSzQ9iK1Y6kNUJVcr6zTnPe?table=tblsdEpLieDoCxtb&view=vewa8sGZrY). 

For the [EASI Leaderboard](https://huggingface.co/spaces/lmms-lab-si/easi-leaderboard), all EASI benchmarks are summarized in [Supported Models & Benchmarks](docs/Support_bench_models.md). A minimal example of recommended --data settings for EASI is:

| Benchmark   | Evaluation settings          |
|-------------|------------------------------|
| [VSI-Bench](https://huggingface.co/datasets/nyu-visionx/VSI-Bench) | [VSI-Bench_32frame](https://huggingface.co/datasets/lmms-lab-si/EASI-Leaderboard-Data/resolve/main/VSI-Bench.tsv)  |
|             |  [VSI-Bench-Debiased_32frame](https://huggingface.co/datasets/lmms-lab-si/EASI-Leaderboard-Data/resolve/main/VSI-Bench-Debiased.tsv)             |
| [MindCube](https://huggingface.co/datasets/MLL-Lab/MindCube)    | [MindCubeBench_tiny_raw_qa](https://huggingface.co/datasets/lmms-lab-si/EASI-Leaderboard-Data/resolve/main/MindCubeBench_tiny_raw_qa.tsv)    |

#### lmms-eval Configuration

**Model Configuration**: lmms-eval supports various model types including `qwen2_5_vl`, `llava`, `internvl2`, and more. Use `--model_args` to specify model parameters like `pretrained`, `attn_implementation`, etc.

**Task Configuration**: Tasks are defined in `lmms-eval/lmms_eval/tasks/`. To list all available tasks:
```bash
lmms-eval --tasks list
```

Example tasks for spatial intelligence evaluation:
| Task Name | Description |
|-----------|-------------|
| `site_bench_image` | SITE-Bench image evaluation |
| `site_bench_video` | SITE-Bench video evaluation |

For more details on lmms-eval usage, refer to the [lmms-eval documentation](lmms-eval/README.md).


### Submision

To submit your evaluation results to our [EASI Leaderboard](https://huggingface.co/spaces/lmms-lab-si/easi-leaderboard):

1. Go to the [EASI Leaderboard](https://huggingface.co/spaces/lmms-lab-si/easi-leaderboard) page.
2. Click **üöÄ Submit here!** to the submission form.
3. Follow the instructions to fill in the submission form, and submit your results.

## ü§ù Contribution

EASI is an open and evolving evaluation suite. We warmly welcome community contributions, including:
- New spatial benchmarks
- New model baselines
- Evaluation tools

If you are interested in contributing, or have questions about integration, please contact us at
üìß easi-lmms-lab@outlook.com


## üñäÔ∏è Citation

```bib
@article{easi2025,
  title={Holistic Evaluation of Multimodal LLMs on Spatial Intelligence},
  author={Cai, Zhongang and Wang, Yubo and Sun, Qingping and Wang, Ruisi and Gu, Chenyang and Yin, Wanqi and Lin, Zhiqian and Yang, Zhitao and Wei, Chen and Shi, Xuanke and Deng, Kewang and Han, Xiaoyang and Chen, Zukai and Li, Jiaqi and Fan, Xiangyu and Deng, Hanming and Lu, Lewei and Li, Bo and Liu, Ziwei and Wang, Quan and Lin, Dahua and Yang, Lei},
  journal={arXiv preprint arXiv:2508.13142},
  year={2025}
}
```
